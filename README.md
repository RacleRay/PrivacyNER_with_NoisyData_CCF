## æ¦‚è§ˆ

![image-20210314114018794](pic/README_pic/image-20210314114018794.png)

æ¨¡å‹ç»“æ„ä¸Šå¯¹ç»“æœå½±å“è¾ƒå¤§çš„åœ°æ–¹æœ‰ï¼š

1. BERT æå–çš„ç‰¹å¾å’Œword embedding ç»„åˆæ—¶ï¼Œéœ€è¦å¯¹åº” Head & Tail positionï¼ŒåŒæ—¶åŠ ä¸€å±‚ layer normï¼Œæ”¶æ•›é€Ÿåº¦ä¼šåŠ å¿«ã€‚
2. ç”±äºæ˜¯ä¸€å±‚FLATç»“æ„ï¼ŒFLATçš„è¾“å‡ºå’Œè¾“å…¥çš„å…³è”æ¯”è¾ƒç›´æ¥ï¼Œéœ€è¦å°†è¾“å…¥ä¸­padçš„ä½ç½®ï¼Œmaskä¸º 0ï¼Œæ•ˆæœä¼šæœ‰ä¸€ç‚¹æå‡ã€‚



å¦å¤–å¯ä»¥è¯•è¯•Relative Position Encodingçš„ä¸åŒèåˆæ–¹å¼ã€‚

```
PrivacyNER_with_NoisyData_CCF
|
â”œâ”€ config.py
â”œâ”€ preprocess.py
â”œâ”€ main.py
â”œâ”€ set_env.sh
â”œâ”€ requirements.txt
â”œâ”€ data
â”‚  â”œâ”€ addition
â”‚  â”œâ”€ dataset.py
â”‚  â””â”€ origin
â”œâ”€ model
â”‚  â”œâ”€ advTrain.py
â”‚  â”œâ”€ layers
â”‚  â”‚  â”œâ”€ attention.py
â”‚  â”‚  â”œâ”€ encoder.py
â”‚  â”‚  â”œâ”€ flat.py
â”‚  â”‚  â”œâ”€ position.py
â”‚  â”‚  â””â”€ structuredOut.py
â”‚  â”œâ”€ model.py
â”‚  â”œâ”€ mymodel.py
â”‚  â””â”€ trainer.py
â”œâ”€ notebook
â”‚  â”œâ”€ data_aug_nlpcda.ipynb
â”‚  â”œâ”€ eda.ipynb
â”‚  â”œâ”€ paddleNLPå·¥å…·.ipynb
â”‚  â”œâ”€ regex.ipynb
â”‚  â””â”€ traditional_model.ipynb
â””â”€ utils
   â”œâ”€ callbacks
   â”‚  â””â”€ earlystop.py
   â”œâ”€ postprocess
   â”‚  â”œâ”€ process.py
   â”‚  â””â”€ selfsupervise.py
   â”œâ”€ preprocess
   â”‚  â”œâ”€ format.py
   â”‚  â”œâ”€ process.py
   â”‚  â””â”€ tokenizer.py
   â””â”€ tools.py
```



## æ¨¡å‹

### FLAT

FLATéƒ¨åˆ†BlogåŸæ–‡ï¼šhttps://mp.weixin.qq.com/s/6aU6ZDYPWPHc3KssuzArKw

è®ºæ–‡ï¼šFLAT: Chinese NER Using Flat-Lattice Transformer

å°†Latticeå›¾ç»“æ„æ— æŸè½¬æ¢ä¸ºæ‰å¹³çš„Flatç»“æ„çš„æ–¹æ³•ï¼Œå¹¶å°†LSTMæ›¿æ¢ä¸ºäº†æ›´å…ˆè¿›çš„Transformer Encoderï¼Œæ›´å¥½åœ°å»ºæ¨¡äº†åºåˆ—çš„**é•¿æœŸä¾èµ–å…³ç³»**ï¼›

æå‡ºäº†ä¸€ç§é’ˆå¯¹Flatç»“æ„çš„**ç›¸å¯¹ä½ç½®ç¼–ç æœºåˆ¶**ï¼Œä½¿å¾—å­—ç¬¦ä¸è¯æ±‡ä¿¡æ¯äº¤äº’æ›´ç›´æ¥ï¼Œåœ¨åŸºäºè¯å…¸çš„ä¸­æ–‡NERæ¨¡å‹ä¸­å–å¾—äº†SOTAã€‚



ç”±äºä¸­æ–‡è¯æ±‡çš„ç¨€ç–æ€§å’Œæ¨¡ç³Šæ€§ï¼ŒåŸºäºå­—ç¬¦çš„åºåˆ—æ ‡æ³¨æ¨¡å‹å¾€å¾€æ¯”åŸºäºè¯æ±‡çš„åºåˆ—æ ‡æ³¨æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œä½†åœ¨åŸºäºå­—ç¬¦çš„æ¨¡å‹ä¸­å¼•å…¥**åˆ†è¯ä¿¡æ¯**å¾€å¾€èƒ½å¤Ÿå¸¦æ¥æ€§èƒ½çš„æå‡ï¼Œå°¤å…¶æ˜¯å¯¹äºNERä»»åŠ¡æ¥è¯´ï¼Œè¯æ±‡èƒ½å¤Ÿæä¾›ä¸°å¯Œçš„å®ä½“è¾¹ç•Œä¿¡æ¯ã€‚

Lattice LSTMé¦–æ¬¡æå‡ºä½¿ç”¨Latticeç»“æ„åœ¨NERä»»åŠ¡ä¸­èå…¥è¯æ±‡ä¿¡æ¯ï¼Œå¦‚å›¾æ‰€ç¤ºï¼Œä¸€ä¸ªå¥å­çš„Latticeç»“æ„æ˜¯ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ªå­—æˆ–è€…ä¸€ä¸ªè¯ã€‚

![image-20210220165643972](pic/README_pic/image-20210220165643972.png)



#### è®¾è®¡é€‚åº”Latticeç»“æ„çš„æ¨¡å‹

![image-20210220165700873](pic/README_pic/image-20210220165700873.png)

Lattice LSTM (ACL 2018): å°†è¯æ±‡ä¿¡æ¯å¼•å…¥ä¸­æ–‡NERçš„å¼€ç¯‡ä¹‹ä½œï¼Œä½œè€…å°†è¯èŠ‚ç‚¹ç¼–ç ä¸ºå‘é‡ï¼Œå¹¶åœ¨å­—èŠ‚ç‚¹ä»¥æ³¨æ„åŠ›çš„æ–¹å¼èåˆè¯å‘é‡ã€‚

Lexicon Rethink CNN(IJCAI 2019): ä½œè€…æå‡ºäº†å«æœ‰rethinkæœºåˆ¶çš„CNNç½‘ç»œè§£å†³Lattice LSTMçš„è¯æ±‡å†²çªé—®é¢˜ã€‚



RNNå’ŒCNNéš¾ä»¥å»ºæ¨¡é•¿è·ç¦»çš„ä¾èµ–å…³ç³»ï¼Œä¸”åœ¨Lattice LSTMä¸­çš„å­—ç¬¦åªèƒ½è·å–å‰å‘ä¿¡æ¯ï¼Œæ²¡æœ‰å’Œè¯æ±‡è¿›è¡Œè¶³å¤Ÿå……åˆ†çš„å…¨å±€äº¤äº’



#### FLAT

[Git Repo](https://github.com/LeeSureman/Flat-Lattice-Transformer)

ä»Transformerçš„position representationå¾—åˆ°å¯å‘ï¼Œä½œè€…ç»™æ¯ä¸€ä¸ªtoken/span(å­—ã€è¯)å¢åŠ äº†ä¸¤ä¸ªä½ç½®ç¼–ç ï¼Œåˆ†åˆ«è¡¨ç¤ºè¯¥spanåœ¨sentenceä¸­å¼€å§‹(head)å’Œç»“æŸ(tail)çš„ä½ç½®

![image-20210220165716666](pic/README_pic/image-20210220165716666.png)

æ‰å¹³çš„ç»“æ„å…è®¸æˆ‘ä»¬ä½¿ç”¨Transformer Encoderï¼Œå…¶ä¸­çš„self-attentionæœºåˆ¶å…è®¸ä»»ä½•å­—ç¬¦å’Œè¯æ±‡è¿›è¡Œç›´æ¥çš„äº¤äº’



#### Relative Position Encoding of Spans

spanæ˜¯å­—ç¬¦å’Œè¯æ±‡çš„æ€»ç§°ï¼Œspanä¹‹é—´å­˜åœ¨ä¸‰ç§å…³ç³»ï¼šäº¤å‰ã€åŒ…å«ã€åˆ†ç¦»ï¼Œç„¶è€Œä½œè€…æ²¡æœ‰ç›´æ¥ç¼–ç è¿™äº›ä½ç½®å…³ç³»ï¼Œè€Œæ˜¯å°†å…¶è¡¨ç¤ºä¸ºä¸€ä¸ªç¨ å¯†å‘é‡ã€‚ä½œè€…ç”¨ å’Œ è¡¨ç¤ºspançš„å¤´å°¾ä½ç½®åæ ‡ï¼Œå¹¶ä»å››ä¸ªä¸åŒçš„è§’åº¦æ¥è®¡ç®— å’Œ çš„è·ç¦»ï¼š

![image-20210220165950494](pic/README_pic/image-20210220165950494.png)

![image-20210220170005373](pic/README_pic/image-20210220170005373.png)

![image-20210220170019854](pic/README_pic/image-20210220170019854.png)

![image-20210220170032945](pic/README_pic/image-20210220170032945.png)

ä½¿ç”¨$A^{*}_{i,j}$ä»£æ›¿ tranformer çš„self attention ä¸­çš„ $A_{i,j}$:

![image-20210220170231288](pic/README_pic/image-20210220170231288.png)

é€šè¿‡FLATæ¨¡å‹åï¼Œå–å‡ºtokençš„ç¼–ç è¡¨ç¤ºï¼Œå°†å…¶é€å…¥CRFå±‚è¿›è¡Œè§£ç å¾—åˆ°é¢„æµ‹çš„æ ‡ç­¾åºåˆ—ã€‚

è®ºæ–‡ä¸­ç»™å‡ºçš„ç»“æœæ˜¾ç¤ºï¼ŒFLATç›¸è¾ƒäºä¸€ä¼—NERæ¨¡å‹ï¼Œå–å¾—äº†SOTAçš„æ•ˆæœã€‚åŒæ—¶ï¼Œä½¿ç”¨è¾ƒå¤§è§„æ¨¡æ•°æ®æ—¶ï¼Œæ•ˆæœæ›´å¥½ã€‚åœ¨å¯¹æ¯”å®éªŒä¸­å‘ç°ï¼Œå­—ç¬¦ä¸åŒ…å«å®ƒçš„è¯æ±‡ä¹‹é—´çš„å……åˆ†äº¤äº’æ˜¯å¾ˆé‡è¦çš„ã€‚

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_size, num_heads, scaled=True, attn_dropout=None):
        super(MultiHeadAttentionRel, self).__init__()

        self.hidden_size = hidden_size

        self.num_heads = num_heads
        self.per_head_size = self.hidden_size // self.num_heads
        self.scaled = scaled
        assert (self.per_head_size * self.num_heads == self.hidden_size)

        # æ­£å¸¸ attention çš„ q,k,v å˜æ¢çŸ©é˜µ
        self.w_k = nn.Linear(self.hidden_size, self.hidden_size)
        self.w_q = nn.Linear(self.hidden_size, self.hidden_size)
        self.w_v = nn.Linear(self.hidden_size, self.hidden_size)

        # è®¡ç®— Rij çš„æƒé‡
        self.w_r = nn.Linear(self.hidden_size, self.hidden_size)

        # è®¡ç®— A* çš„æƒé‡
        self.u = nn.Parameter(torch.randn(self.num_heads, self.per_head_size), requires_grad=True)
        self.v = nn.Parameter(torch.randn(self.num_heads, self.per_head_size), requires_grad=True)

        self.dropout = nn.Dropout(attn_dropout)

    def forward(self, key, query, value, pos, flat_mask):
        "pos ä¸º è‡ªå®šä¹‰çš„ postion embeddingï¼Œå¯¹åº”å…¬å¼çš„ Rij"
        batch = key.size(0)

        # è¾“å…¥çº¿æ€§å˜æ¢
        key = self.w_k(key)
        query = self.w_q(query)
        value = self.w_v(value)
        rel_pos_embedding = self.w_r(pos)

        ####### è®¡ç®— A* çŸ©é˜µçš„æ–¹æ³• å’Œ è®ºæ–‡ä¸æ˜¯å®Œå…¨ä¸€è‡´
        # batch, seq_len, n_head, d_head
        key = torch.reshape(key, [batch, -1, self.num_heads, self.per_head_size])
        query = torch.reshape(query, [batch, -1, self.num_heads, self.per_head_size])
        value = torch.reshape(value, [batch, -1, self.num_heads, self.per_head_size])
        # batch, seq_len, seq_len, n_head, d_head
        rel_pos_embedding = torch.reshape(rel_pos_embedding,
                                          list(rel_pos_embedding.size()[:3]) + [self.num_heads, self.per_head_size])

        # batch, n_head, seq_len, d_head
        key = key.transpose(1, 2)
        query = query.transpose(1, 2)
        value = value.transpose(1, 2)

        # batch, n_head, d_head, seq_len
        key = key.transpose(-1, -2)

        # 1, num_heads, 1, d_head
        u_for_c = self.u.unsqueeze(0).unsqueeze(-2)

        # batch, n_head, seq_len, d_head
        query_and_u_for_c = query + u_for_c

        # batch, n_head, seq_len, seq_len
        A_C = torch.matmul(query_and_u_for_c, key)

        # batch, n_head, seq_len, d_head, seq_len
        rel_pos_embedding_for_b = rel_pos_embedding.permute(0, 3, 1, 4, 2)
        # batch, n_head, seq_len, seq_len, 1, d_head
        query_for_b = query.view([batch, self.num_heads, query.size(2), 1, self.per_head_size])
        # batch, n_head, seq_len, seq_len, 1, d_head
        query_for_b_and_v_for_d = query_for_b + self.v.view(1, self.num_heads, 1, 1, self.per_head_size)

        # batch, n_head, seq_len, seq_len
        B_D = torch.matmul(query_for_b_and_v_for_d, rel_pos_embedding_for_b).squeeze(-2)

        # batch, n_head, seq_len, seq_len
        attn_score_raw = A_C + B_D

        # è®¡ç®— score
        if self.scaled:
            attn_score_raw = attn_score_raw / math.sqrt(self.per_head_size)

        mask = 1 - flat_mask.long().unsqueeze(1).unsqueeze(1)
        attn_score_raw_masked = attn_score_raw.masked_fill(mask.bool(), -1e15)

        # batch, n_head, seq_len, seq_len
        attn_score = F.softmax(attn_score_raw_masked, dim=-1)
        attn_score = self.dropout(attn_score)

        # batch, n_head, seq_len, d_head
        value_weighted_sum = torch.matmul(attn_score, value)
        # batch, seq_len, n_head, d_head -> batch, seq_len, hidden_size
        result = value_weighted_sum.transpose(1, 2).contiguous().reshape(batch, -1, self.hidden_size)

        return result
```



### BERT

æ•™ç¨‹åšå®¢å¾ˆå¤šï¼Œæ¯”å¦‚ http://jalammar.github.io/illustrated-bert/



### CRF

å‚è€ƒ https://racleray.github.io/2020/11/18/CRF-SimpleNote/, https://racleray.github.io/2021/02/22/%E6%B5%85%E6%B6%89%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/



### MRC

è®ºæ–‡ï¼šA Unified MRC Framework for Named Entity Recognition

[Git Repo](https://github.com/ShannonAI/mrc-for-flat-nested-ner?utm_source=catalyzex.com)

è½¬æ¢ä¸ºé˜…è¯»ç†è§£ï¼ˆMRCï¼‰ä»»åŠ¡ï¼Œæ¥è§£å†³NERé—®é¢˜ã€‚ä¼¼ä¹æœ‰å¾ˆå¤šæç ”ç©¶çš„ï¼Œéƒ½åœ¨å°è¯•å°†NLPé—®é¢˜è½¬æ¢åˆ°MRCæ¡†æ¶ä¸‹ï¼Œè§£å†³é—®é¢˜ã€‚

ç›®çš„ï¼Œè§£å†³NERä¸­çš„å®ä½“é‡å ã€åµŒå¥—å…³ç³»é—®é¢˜ã€‚è¿™æ˜¯åºåˆ—å»ºæ¨¡æ–¹å¼ï¼Œæ¯”è¾ƒéš¾å¤„ç†çš„é—®é¢˜ã€‚

æ•°æ®ï¼Œå¤„ç†ä¸ºä¸‰å…ƒç»„å½¢å¼ï¼š(é—®é¢˜ï¼Œç­”æ¡ˆï¼Œä¸Šä¸‹æ–‡)

> å…¶ä¸­ï¼Œé—®é¢˜ï¼šä¸€æ®µå¯¹ å®ä½“ç±»å‹ çš„æè¿°æ–‡å­—ï¼Œå¤šç§å®ä½“ï¼Œå°±æœ‰å¤šä¸ªé—®é¢˜ï¼›ç­”æ¡ˆï¼šä¸º å®ä½“çš„èµ·å§‹ indexï¼›ä¸Šä¸‹æ–‡å°±æ˜¯å¾…è¯†åˆ«çš„æ•´ä¸ªæ–‡æœ¬ã€‚

æ¨¡å‹ï¼Œä½¿ç”¨BERTï¼š

![image-20210220173026930](pic/README_pic/image-20210220173026930.png)

æ¯ä¸ªtokené¢„æµ‹è¾“å‡ºæœ‰ä¸¤ä¸ªï¼Œæ˜¯å¦ä¸ºå®ä½“å¼€å§‹å­—ï¼Œæ˜¯å¦ä¸ºå®ä½“ç»“æŸå­—ã€‚

![image-20210220173255967](pic/README_pic/image-20210220173255967.png)

è¾“å‡ºä¸º 2 ç»´ï¼Œæ˜¯å’Œä¸æ˜¯çš„é¢„æµ‹æ¦‚ç‡ã€‚åˆ†åˆ«å¯¹æ¯ä¸ªä½ç½®åˆ¤æ–­ï¼Œæ˜¯å¦ä¸ºå¼€å§‹å­—æˆ–è€…ç»“æŸå­—ã€‚

![image-20210220174003119](pic/README_pic/image-20210220174003119.png)

ä½†æ˜¯è¿™ä¸ªä¸¤ä¸ªé›†åˆï¼Œåœ¨æœ‰ç›‘ç£æ•°æ®æ¡ä»¶ä¸‹ï¼Œå³è®­ç»ƒæ—¶ï¼Œå¹¶æ²¡æœ‰å¿…è¦ï¼Œåªåœ¨é¢„æµ‹æ¨æ–­æ—¶ä½¿ç”¨ï¼ˆæ¨æ–­éœ€è¦é€šè¿‡ä¸‹å¼è®¡ç®—æ‰€æœ‰ç»„åˆçš„æ¦‚ç‡ Pï¼‰ã€‚å› ä¸ºä¸‹å¼ï¼š

![image-20210220174208145](pic/README_pic/image-20210220174208145.png)

ç›´æ¥æ ¹æ®æ ‡æ³¨æ•°æ®çš„ i, j å¯¹æ ‡æ³¨éƒ¨åˆ†è®¡ç®— Pã€‚è€Œä¸ç”¨å¯¹æ‰€æœ‰ i, j ç»„åˆç®—ä¸€æ¬¡ Pã€‚

æŸå¤±ï¼Œå¤šä¸ªé¢„æµ‹æŸå¤±ä¹‹å’Œï¼š

![image-20210220174356970](pic/README_pic/image-20210220174356970.png)

![image-20210220174404314](pic/README_pic/image-20210220174404314.png)

![image-20210220174417043](pic/README_pic/image-20210220174417043.png)

æƒé‡ä¸ºè¶…å‚æ•°ã€‚



### Simple-Lexicon

è®ºæ–‡ï¼šSimple-Lexiconï¼šSimplify the Usage of Lexicon in Chinese NER

[Git Repo](https://github.com/v-mipeng/LexiconAugmentedNER?utm_source=catalyzex.com)

åœ¨Embeddingä¿¡æ¯çš„è¾“å…¥ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œå°è¯•äº†å¤šç§æ–¹å¼ã€‚

> 1. Softwordï¼šä½¿ç”¨åˆ†è¯å·¥å…·ï¼Œæ ‡è®°è¯çš„ BMESOï¼Œç»“åˆå­—å‘é‡å’Œæ ‡è®°å‘é‡è¾“å…¥ã€‚å­˜åœ¨è¯¯å·®ä¼ æ’­é—®é¢˜ï¼Œæ— æ³•å¼•å…¥ä¸€æ•´ä¸ªè¯æ±‡å¯¹åº”word embedding
>
> 2. ExtendSoftwordï¼šç»„åˆæ‰€æœ‰å­—çš„æ‰€æœ‰BMEï¼Œå¾—åˆ°å¯èƒ½çš„è¯ï¼Œä½†æ˜¯æ— æ³•å¤åŸåŸå§‹çš„è¯æ±‡ä¿¡æ¯æ˜¯æ€æ ·
>
> 3. Soft-lexiconï¼šå¯¹å½“å‰å­—ç¬¦ï¼Œä¾æ¬¡è·å–BMESå¯¹åº”æ‰€æœ‰è¯æ±‡é›†åˆã€‚
>
>    <img src="pic/README_pic/image-20210220182059909.png" alt="image-20210220182059909" style="zoom:67%;" />
>
>    æ ¹æ®è¯é¢‘åŠ æƒè¯å‘é‡ï¼Œä¸å­—å‘é‡æ±‚å’Œã€‚

è¯¥æ¨¡å‹æ¯”Lattice LSTM, WC-LSTMç­‰ï¼Œåœ¨è¾“å…¥embeddingä¸Šè¿›è¡Œæ”¹è¿›çš„æ¨¡å‹ï¼Œæ•ˆæœæ›´å¥½ï¼Œæ›´å®¹æ˜“ä½¿ç”¨å’Œè¿ç§»ã€‚



## ç­–ç•¥

### Positive-unlabeled learning -- PU Learning

> åœ¨åªæœ‰æ­£ç±»å’Œæ— æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒäºŒåˆ†ç±»å™¨

> Method 1   Directly 
>
> 1. å°†æ­£æ ·æœ¬å’Œéƒ¨åˆ†ç­›é€‰å‡ºçš„æœªæ ‡è®°æ ·æœ¬åˆ†åˆ«çœ‹ä½œæ˜¯positive sampleså’Œnegative samples
> 2. è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œè¾“å‡ºæ ·æœ¬å±äºæ­£ã€è´Ÿç±»çš„æ¦‚ç‡
> 3. ä½¿ç”¨è®­ç»ƒå¥½çš„åˆ†ç±»å™¨ã€‚åˆ†ç±»æœªæ ‡æ³¨æ•°æ®ï¼Œè‹¥æ­£ç±»çš„æ¦‚ç‡ å¤§äº è´Ÿç±»çš„æ¦‚ç‡ï¼Œåˆ™è¯¥æœªæ ‡æ³¨æ ·æœ¬çš„æ›´å¯èƒ½ä¸ºæ­£ç±»
>
> 
>
> Method 2   PU bagging
>
> 1. å°†æ‰€æœ‰æ­£æ ·æœ¬å’Œæœªæ ‡è®°æ ·æœ¬è¿›è¡Œéšæœºç»„åˆ bootstrap æ¥åˆ›å»ºè®­ç»ƒé›†ï¼›
> 2. å°†æ­£æ ·æœ¬å’Œæœªæ ‡è®°æ ·æœ¬è§†ä¸ºpositiveå’Œnegativeï¼Œè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼›
> 3. å°†åˆ†ç±»å™¨åº”ç”¨äºä¸åœ¨è®­ç»ƒé›†ä¸­çš„æœªæ ‡è®°æ ·æœ¬ OOBï¼ˆâ€œout of bagâ€ï¼‰ï¼Œå¹¶è®°å½•å…¶åˆ†æ•°ï¼›
> 4. é‡å¤ä¸Šè¿°ä¸‰ä¸ªæ­¥éª¤ï¼Œæœ€åæ¯ä¸ªæœªæ ‡è®°æ ·æœ¬çš„åˆ†æ•°ä¸ºæ¯ä¸€è½® OOBåˆ†æ•° çš„å¹³å‡å€¼ã€‚
>
> 
>
> Method 3
>
> äººå·¥æ ‡æ³¨ä¸€éƒ¨åˆ†ç¡®è®¤ä¸ºè´Ÿç±»çš„æ•°æ®ï¼Œè®­ç»ƒåˆ†ç±»å™¨è¯†åˆ«è¿™äº› ç¡®è®¤ä¸º è´Ÿç±»çš„æ•°æ®ã€‚
>
> 
>
> [ç¤ºä¾‹](https://github.com/phuijse/bagging_pu/blob/master/PU_Learning_simple_example.ipynb) [ç¤ºä¾‹](https://github.com/roywright/pu_learning/blob/master/circles.ipynb)

è®ºæ–‡ï¼šDistantly Supervised Named Entity Recognition using Positive-Unlabeled Learningï¼Œå°†PU Learningåº”ç”¨åœ¨NERä»»åŠ¡ä¸Š [Git Repo](https://github.com/v-mipeng/LexiconNER)ï¼š

> 1. é¦–å…ˆæœ‰ æœªæ ‡è®°æ•°æ® Duï¼Œå®ä½“å­—å…¸ Dictï¼›
>
> 2. ä½¿ç”¨æœ€å¤§åŒ¹é…æ–¹æ³•ï¼Œæ ‡è®°ä¸€éƒ¨åˆ† Duï¼Œæ˜¯NEåˆ™ä¸ºæ­£ç±»ï¼Œä¸æ˜¯NEåˆ™ä¸ºè´Ÿç±»ï¼›
>
>    <img src="pic/README_pic/image-20210220164737197.png" alt="image-20210220164737197" style="zoom:67%;" />
>
> 3. å¯¹æ¯ä¸€ç§NEç±»å‹ï¼ˆæ¯”å¦‚ï¼ŒLocï¼ŒNaneï¼‰è®­ç»ƒä¸€ä¸ªPU åˆ†ç±»å™¨ï¼ˆè‡ªå®šä¹‰çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼‰ï¼›
>
> 4. ä½¿ç”¨å¤šä¸ªPU åˆ†ç±»å™¨ï¼Œå¯¹å‰©ä½™çš„ Duï¼Œè¿›è¡Œé¢„æµ‹ï¼Œæ¯ä¸€ä¸ªè¯ï¼Œå–é¢„æµ‹æ¦‚ç‡æœ€å¤§çš„é‚£ä¸€ç±»æ ‡è®°ï¼›
>
> 5. è‹¥æŸäº› è¯ å¤šæ¬¡è¢«é¢„æµ‹ä¸º å®ä½“ï¼Œä¸”æ¯æ¬¡å‡ºç°éƒ½è¢«é¢„æµ‹ä¸ºåŒä¸€ç±»å®ä½“ï¼Œé‚£ä¹ˆï¼Œå°†è¿™ä¸ªè¯ï¼ŒåŠ å…¥Dictï¼›
>
> 6. é‡å¤ä»¥ä¸Šæ­¥éª¤ï¼Œç›´åˆ°Dictä¸å†æ”¹å˜ã€‚

æ–¹æ³•æ˜¯è¿™ä¹ˆä¸ªæ–¹æ³•ï¼Œå®é™…ä¸Šä»£ç å¹¶æ²¡æœ‰å¦‚æ­¤å®ç°ğŸ˜…ã€‚ä»£ç é‡Œé¢çš„ loss mask çš„ä½¿ç”¨æ–¹å¼ï¼Œä½¿ç”¨å¤šä¸ªk flod çš„å­æ¨¡å‹é‡æ–°å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œé¢„æµ‹æŠ•ç¥¨ï¼Œå¹¶åˆ é™¤å¾—ç¥¨æ•°å°‘äºé˜ˆå€¼çš„æ ‡è®°ã€‚ç›®çš„æ˜¯ä¸ºäº†å‡å°‘å™ªå£°æ•°æ®ã€‚è¿™å’Œæˆ‘æŸ¥åˆ°çš„PU learningçš„æ€æƒ³æ˜¯ä¸ä¸€è‡´çš„ã€‚

å¦å¤–ï¼Œè¿™ç§æŠ•ç¥¨çš„æ–¹æ³•å¯ä»¥ç”¨åœ¨ test data ä¸­ï¼Œä½†æ˜¯æ˜¯æå– å¾—ç¥¨æ•°é«˜äºé˜ˆå€¼çš„ æ ·ä¾‹å’Œé¢„æµ‹æ ‡è®°ã€‚åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­ï¼Œå¦‚æ­¤è¿™èˆ¬ åŠç›‘ç£å­¦ä¹ ï¼Œæ˜¯ä¸æ˜¯æ›´niceï¼Ÿå—¯ï¼Œæƒ³æ³•ä¸é”™ã€‚



### FGM

[å¼•ç”¨BlogåŸæ–‡](https://zhuanlan.zhihu.com/p/91269728)

å¯¹æŠ—å¯ä»¥ä½œä¸ºä¸€ç§é˜²å¾¡æœºåˆ¶ï¼Œå¹¶ä¸”ç»è¿‡ç®€å•çš„ä¿®æ”¹ï¼Œä¾¿èƒ½ç”¨åœ¨NLPä»»åŠ¡ä¸Šï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å¯¹æŠ—è®­ç»ƒå¯ä»¥å†™æˆä¸€ä¸ªæ’ä»¶çš„å½¢å¼ï¼Œç”¨å‡ è¡Œä»£ç å°±å¯ä»¥åœ¨è®­ç»ƒä¸­è‡ªç”±åœ°è°ƒç”¨ã€‚

åœ¨åŸå§‹è¾“å…¥æ ·æœ¬ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=x) ä¸ŠåŠ ä¸€ä¸ªæ‰°åŠ¨ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=+r_%7Badv%7D) ï¼Œå¾—åˆ°å¯¹æŠ—æ ·æœ¬åï¼Œç”¨å…¶è¿›è¡Œè®­ç»ƒã€‚å°†è¾“å…¥æ ·æœ¬å‘ç€æŸå¤±ä¸Šå‡çš„æ–¹å‘å†è¿›ä¸€æ­¥ï¼Œå¾—åˆ°çš„å¯¹æŠ—æ ·æœ¬å°±èƒ½é€ æˆæ›´å¤§çš„æŸå¤±ï¼Œæé«˜æ¨¡å‹çš„é”™è¯¯ç‡ã€‚é—®é¢˜å¯ä»¥è¢«æŠ½è±¡æˆè¿™ä¹ˆä¸€ä¸ªæ¨¡å‹ï¼š

![[å…¬å¼]](https://www.zhihu.com/equation?tex=+%5Cmin_%7B%5Ctheta%7D-%5Clog+P%28y%7Cx%2Br_%7Badv%7D%3B%5Ctheta%29+)

å…¶ä¸­ï¼Œ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=y) ä¸ºgold labelï¼Œ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Ctheta) ä¸ºæ¨¡å‹å‚æ•°ã€‚Goodfellowè®¤ä¸ºï¼Œç¥ç»ç½‘ç»œç”±äºå…¶çº¿æ€§çš„ç‰¹ç‚¹ï¼Œå¾ˆå®¹æ˜“å—åˆ°çº¿æ€§æ‰°åŠ¨çš„æ”»å‡»ã€‚äºæ˜¯ï¼Œä»–æå‡ºäº† Fast Gradient Sign Method (FGSM) ï¼š

![[å…¬å¼]](https://www.zhihu.com/equation?tex=r_%7Badv%7D+%3D+%5Cepsilon+%5Ccdot+%5Ctext%7Bsgn%7D%28%5Ctriangledown_x+L%28%5Ctheta%2C+x%2C+y%29%29)

å…¶ä¸­ï¼Œ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Ctext%7Bsgn%7D) ä¸ºç¬¦å·å‡½æ•°ï¼Œ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=L) ä¸ºæŸå¤±å‡½æ•°ã€‚Goodfellowå‘ç°ï¼Œä»¤ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Cepsilon%3D0.25) ï¼Œç”¨è¿™ä¸ªæ‰°åŠ¨èƒ½ç»™ä¸€ä¸ªå•å±‚åˆ†ç±»å™¨é€ æˆ99.9%çš„é”™è¯¯ç‡ã€‚

Goodfellowè¿˜æ€»ç»“äº†å¯¹æŠ—è®­ç»ƒçš„ä¸¤ä¸ªä½œç”¨ï¼š

1. æé«˜æ¨¡å‹åº”å¯¹æ¶æ„å¯¹æŠ—æ ·æœ¬æ—¶çš„é²æ£’æ€§ï¼›
2. ä½œä¸ºä¸€ç§regularizationï¼Œå‡å°‘overfittingï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚

ä»ä¼˜åŒ–çš„è§†è§’ï¼Œé—®é¢˜é‡æ–°å®šä¹‰æˆäº†ä¸€ä¸ªæ‰¾éç‚¹çš„é—®é¢˜ï¼ŒMin-Maxï¼šå†…éƒ¨æŸå¤±å‡½æ•°çš„æœ€å¤§åŒ–ï¼Œå¤–éƒ¨ç»éªŒé£é™©çš„æœ€å°åŒ–ï¼š

![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Cmin_%5Ctheta+%5Cmathbb%7BE%7D_%7B%28x%2C+y%29%5Csim+%5Cmathcal%7BD%7D%7D+%5B%5Cmax_%7Br_%7Badv%7D+%5Cin+%5Cmathcal%7BS%7D%7D+L%28%5Ctheta%2C+x%2Br_%7Badv%7D%2C+y%29%5D)

1. å†…éƒ¨maxæ˜¯ä¸ºäº†æ‰¾åˆ°worst-caseçš„æ‰°åŠ¨ï¼Œä¹Ÿå°±æ˜¯æ”»å‡»ï¼Œå…¶ä¸­ï¼Œ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=L) ä¸ºæŸå¤±å‡½æ•°ï¼Œ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BS%7D) ä¸ºæ‰°åŠ¨çš„èŒƒå›´ç©ºé—´ã€‚
2. å¤–éƒ¨minæ˜¯ä¸ºäº†åŸºäºè¯¥æ”»å‡»æ–¹å¼ï¼Œæ‰¾åˆ°æœ€é²æ£’çš„æ¨¡å‹å‚æ•°ï¼Œä¹Ÿå°±æ˜¯é˜²å¾¡ï¼Œå…¶ä¸­ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D) æ˜¯è¾“å…¥æ ·æœ¬çš„åˆ†å¸ƒã€‚

CVä»»åŠ¡çš„è¾“å…¥æ˜¯è¿ç»­çš„RGBçš„å€¼ï¼Œè€ŒNLPé—®é¢˜ä¸­ï¼Œè¾“å…¥æ˜¯ç¦»æ•£çš„å•è¯åºåˆ—ï¼Œä¸€èˆ¬ä»¥one-hot vectorçš„å½¢å¼å‘ˆç°ï¼Œå¦‚æœç›´æ¥åœ¨raw textä¸Šè¿›è¡Œæ‰°åŠ¨ï¼Œé‚£ä¹ˆæ‰°åŠ¨çš„å¤§å°å’Œæ–¹å‘å¯èƒ½éƒ½æ²¡ä»€ä¹ˆæ„ä¹‰ã€‚Goodfellowåœ¨17å¹´çš„[ICLR](https://arxiv.org/abs/1605.07725)ä¸­æå‡ºäº†å¯ä»¥åœ¨è¿ç»­çš„embeddingä¸Šåšæ‰°åŠ¨ã€‚åœ¨CVä»»åŠ¡ï¼Œæ ¹æ®ç»éªŒæ€§çš„ç»“è®ºï¼Œå¯¹æŠ—è®­ç»ƒå¾€å¾€ä¼šä½¿å¾—æ¨¡å‹åœ¨éå¯¹æŠ—æ ·æœ¬ä¸Šçš„è¡¨ç°å˜å·®ï¼Œç„¶è€Œç¥å¥‡çš„æ˜¯ï¼Œåœ¨NLPä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›åè€Œå˜å¼ºäº†ã€‚

å› æ­¤ï¼Œ**åœ¨NLPä»»åŠ¡ä¸­ï¼Œå¯¹æŠ—è®­ç»ƒçš„è§’è‰²ä¸å†æ˜¯ä¸ºäº†é˜²å¾¡åŸºäºæ¢¯åº¦çš„æ¶æ„æ”»å‡»ï¼Œåè€Œæ›´å¤šçš„æ˜¯ä½œä¸ºä¸€ç§regularizationï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›**ã€‚



å¯¹æŠ—è®­ç»ƒï¼ŒFSGMçš„ä¿®æ”¹ç‰ˆæœ¬ï¼Œå–æ¶ˆäº†ç¬¦å·å‡½æ•°ï¼Œå¯¹æ¢¯åº¦è®¡ç®—è¿›è¡Œscaleï¼Œè€Œä¸æ˜¯åªä½¿ç”¨ +1 æˆ–è€… -1 ä»£æ›¿ã€‚

> 1. åŸç½‘ç»œè¿›è¡Œä¸€æ¬¡ï¼Œå‰å‘åå‘ä¼ æ’­ï¼Œå¾—åˆ°æ¢¯åº¦g
>
> 2. è®¡ç®—embeddingçŸ©é˜µçš„ä¿®æ­£æ¢¯åº¦ r:
>
>    $r=\frac{\epsilon g}{\|g\|_{2}}$
>
> 3. è¾“å…¥ embedding + r ï¼Œè®¡ç®—å¯¹æŠ—æ¢¯åº¦ ga
>
> 4. å°† ga ç´¯åŠ åˆ° g ä¸­ï¼Œå¾—åˆ° gf
>
> 5. æ¢å¤åŸç½‘ç»œçš„embeddingæ•°å€¼ï¼Œä½¿ç”¨ gf å¯¹å‚æ•°è¿›è¡Œæ›´æ–°



**Projected Gradient Descentï¼ˆPGDï¼‰**ï¼š**â€œå°æ­¥èµ°ï¼Œå¤šèµ°å‡ æ­¥â€**ï¼Œå¦‚æœèµ°å‡ºäº†æ‰°åŠ¨åŠå¾„ä¸º ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Cepsilon) çš„ç©ºé—´ï¼Œå°±æ˜ å°„å›â€œçƒé¢â€ä¸Šï¼Œä»¥ä¿è¯æ‰°åŠ¨ä¸è¦è¿‡å¤§ã€‚

![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+x_%7Bt%2B1%7D+%26%3D+%5CPi_%7Bx%2B%5Cmathcal%7BS%7D%7D%28x_t%2B%5Calpha+g%28x_t%29%2F%7C%7Cg%28x_t%29%7C%7C_2%29+%5C%5C+g%28x_t%29+%26%3D+%5Ctriangledown_x+L%28%5Ctheta%2C+x_t%2C+y%29+%5Cend%7Balign%7D+)

å…¶ä¸­ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BS%7D%3D%7Br%5Cin%5Cmathbb%7BR%7D%5Ed%3A%7C%7Cr%7C%7C_2+%5Cleq+%5Cepsilon%7D) ä¸ºæ‰°åŠ¨çš„çº¦æŸç©ºé—´ï¼Œ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Calpha) ä¸ºå°æ­¥çš„æ­¥é•¿ã€‚

PGDæ¨¡å‹èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª**éå¸¸ä½ä¸”é›†ä¸­çš„lossåˆ†å¸ƒ**ã€‚

å¦å¤–åœ¨åŠç›‘ç£æ¡ä»¶ä¸‹ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å¯¹æŠ—è®­ç»ƒæ–¹æ³•Virtual Adversarial Trainingè¿›è¡ŒåŠç›‘ç£è®­ç»ƒã€‚



ç¤ºä¾‹ä»£ç 

```python
import torch


grad_backup = {}


def save_grad(tensorName):
    def backward_hook(grad: torch.Tensor):
        grad_backup[tensorName] = grad

    return backward_hook


class PGD:
    def __init__(self, model):
        self.model = model
        self.emb_backup = {}

    def attack(self,
               epsilon=1.,
               alpha=0.3,
               emb_name='emb.',
               is_first_attack=False):
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                if is_first_attack:
                    self.emb_backup[name] = param.data.clone()
                norm = torch.norm(param.grad)
                if norm != 0 and not torch.isnan(norm):
                    r_at = alpha * param.grad / norm
                    param.data.add_(r_at)
                    param.data = self.project(name, param.data, epsilon)

    def restore(self, emb_name='emb.'):
        # emb_nameè¿™ä¸ªå‚æ•°è¦æ¢æˆä½ æ¨¡å‹ä¸­embeddingçš„å‚æ•°å
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                assert name in self.emb_backup
                param.data = self.emb_backup[name]
        self.emb_backup = {}

    def project(self, param_name, param_data, epsilon):
        r = param_data - self.emb_backup[param_name]
        if torch.norm(r) > epsilon:
            r = epsilon * r / torch.norm(r)
        return self.emb_backup[param_name] + r

    def backup_grad(self):
        # æ­¤å¤„ä¹Ÿå¯ä»¥ç›´æ¥ç”¨ä¸€ä¸ªæˆå‘˜å˜é‡å‚¨å­˜ gradï¼Œè€Œä¸ç”¨ register_hook å­˜å‚¨åœ¨å…¨å±€å˜é‡ä¸­
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.register_hook(save_grad(name))

    def restore_grad(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.grad = grad_backup[name]



if __name__ == '__main__':
    # ç¤ºä¾‹è¿‡ç¨‹
    pgd = PGD(model)
    K = 3 # å°æ­¥èµ°çš„æ­¥æ•°
    for batch_input, batch_label in data:
        # æ­£å¸¸è®­ç»ƒ
        loss = model(batch_input, batch_label)
        loss.backward() # åå‘ä¼ æ’­ï¼Œå¾—åˆ°æ­£å¸¸çš„grad
        pgd.backup_grad()

        # å¯¹æŠ—è®­ç»ƒ
        for t in range(K):
            pgd.attack(is_first_attack=(t==0)) # åœ¨embeddingä¸Šæ·»åŠ å¯¹æŠ—æ‰°åŠ¨, first attackæ—¶å¤‡ä»½param.data
            if t != K-1:
                model.zero_grad()
            else:
                pgd.restore_grad()
            loss_adv = model(batch_input, batch_label)
            loss_adv.backward() # åå‘ä¼ æ’­ï¼Œå¹¶åœ¨æ­£å¸¸çš„gradåŸºç¡€ä¸Šï¼Œç´¯åŠ å¯¹æŠ—è®­ç»ƒçš„æ¢¯åº¦
        pgd.restore() # æ¢å¤embeddingå‚æ•°

        # æ¢¯åº¦ä¸‹é™ï¼Œæ›´æ–°å‚æ•°
        optimizer.step()
        model.zero_grad()
```

```python
import torch


class FGM:
    def __init__(self, model):
        self.model = model
        self.backup = {}

    def attack(self, epsilon=1, emb_name='emb.'):
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                self.backup[name] = param.data.clone()
                norm = torch.norm(param.grad)
                if norm != 0 and not torch.isnan(norm):
                    r_adv = epsilon * param.grad / norm
                    param.data.add_(r_adv)

    def restore(self, emb_name='emb.'):
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}


if __name__ == "__main__":
    # ç¤ºä¾‹è¿‡ç¨‹
    fgm = FGM(model)
    for batch_input, batch_label in data:
        # æ­£å¸¸è®­ç»ƒ
        loss = model(batch_input, batch_label)
        loss.backward()  # åå‘ä¼ æ’­ï¼Œå¾—åˆ°æ­£å¸¸çš„grad
        # å¯¹æŠ—è®­ç»ƒ
        fgm.attack()  # åœ¨embeddingä¸Šæ·»åŠ å¯¹æŠ—æ‰°åŠ¨
        loss_adv = model(batch_input, batch_label)
        loss_adv.backward()  # åå‘ä¼ æ’­ï¼Œå¹¶åœ¨æ­£å¸¸çš„gradåŸºç¡€ä¸Šï¼Œç´¯åŠ å¯¹æŠ—è®­ç»ƒçš„æ¢¯åº¦
        fgm.restore()  # æ¢å¤embeddingå‚æ•°
        # æ¢¯åº¦ä¸‹é™ï¼Œæ›´æ–°å‚æ•°
        optimizer.step()
        model.zero_grad()
```



### SWA

Stochastic Weight Averagingï¼Œæ–¹æ³•çš„æå‡ºè€…è®¤ä¸ºï¼Œè®­ç»ƒæœŸé—´å¾—åˆ°çš„å±€éƒ¨æœ€å°å€¼ å€¾å‘äº åœ¨æŸå¤±å€¼è¾ƒä½çš„åŒºåŸŸçš„è¾¹ç•Œï¼Œè€Œä¸æ˜¯é›†ä¸­åœ¨æŸå¤±æ›´ä½çš„åŒºåŸŸä¸­å¿ƒéƒ¨åˆ†ã€‚æ‰€ä»¥ï¼ŒStochastic Weight Averagingå¯ä»¥é€šè¿‡å¯¹è¾¹ç•Œçš„å¹³å‡ï¼Œå¾—åˆ°æ›´å¥½æ€§èƒ½å’Œæ›´å¥½æ³›åŒ–æ€§èƒ½çš„æ¨¡å‹ã€‚[Git Repo](https://github.com/timgaripov/swa)

> 1. ä¿å­˜ä¸¤å¥—æƒé‡w, wswaï¼›
>
> 2. ä½¿ç”¨å¾ªç¯å­¦ä¹ ç‡ï¼Œè®­ç»ƒwï¼›
>
> 3. è¾¾åˆ°æŒ‡å®šè½®æ¬¡ï¼Œæ›´æ–°wsï¼Œ$n_{models}$æŒ‡æ›´æ–°$w_{swa}$æ—¶ï¼Œä¸­é—´é—´éš”çš„è½®æ¬¡:
>
>    $w_{swa} = \frac{w_{swa}n_{models}+w}{n_{models}+1}$
>
> 4. å¾ªç¯ä»¥ä¸Šæ­¥éª¤ï¼Œæœ€åä½¿ç”¨wswaï¼Œä½œä¸ºæœ€ç»ˆæ¨¡å‹

æœ‰å¯ä»¥ç›´æ¥ä½¿ç”¨çš„å·¥å…·ï¼Œæ¯”è¾ƒæ–¹ä¾¿ã€‚~*from* torchcontrib.optim *import* SWA~

```python
optimizer = torch.optim.Adam(params_lr)
# Stochastic Weight Averaging
optimizer = SWA(optimizer)


if ...:
    optimizer.update_swa()
    
...
# è®­ç»ƒç»“æŸæ—¶ä½¿ç”¨æ”¶é›†åˆ°çš„swa moving average
optimizer.swap_swa_sgd()
# optimizer.bn_update(
#     train_dataloader,
#     model)  # æ›´æ–°BatchNormçš„ running mean

# save
```



å‚è€ƒé“¾æ¥ï¼š

[2020CCF-NER](https://github.com/BaberMuyu/2020CCF-NER)

[Flat-Lattice-Transformer](https://github.com/LeeSureman/Flat-Lattice-Transformer)
